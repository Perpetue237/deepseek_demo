{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model and tokenizer from Huggingface....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c8c29ef26e9432dba25819995085ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.87k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "080451ca31be43a78bf368a9c945962b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.37M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c401fb71e90b4319bf6a733215b27caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/631 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26d2caa108c4c2d849edb7df5d3bced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.69G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cb1d1fd1ea0491688a896c322237934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer downloaded and save to:  models/deepseek-ai/deepseek-coder-1.3b-instruct\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os, torch\n",
    "\n",
    "model_name = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
    "model_path = os.path.join(\"models\", model_name)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    print(\"Downloading model and tokenizer from Huggingface....\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\n",
    "    model.save_pretrained(model_path, from_pt=True)\n",
    "    tokenizer.save_pretrained(model_path)\n",
    "    print(\"Model and tokenizer downloaded and save to: \", model_path)\n",
    "else:\n",
    "    print(\"Loading model and tokenizer from path!\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path).cuda()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a python code to sort a list!\n",
      "\n",
      "\n",
      "# Python program to sort a list\n",
      "\n",
      "# Function to sort the list\n",
      "def sort_list(input_list):\n",
      "    # Using built-in sort function\n",
      "    input_list.sort()\n",
      "    return input_list\n",
      "\n",
      "# Test the function\n",
      "numbers = [64, 34, 25, 12, 22, 11, 90]\n",
      "print(\"Original list: \", numbers)\n",
      "print(\"Sorted list: \", sort_list(numbers))\n",
      "\n",
      "# Output:\n",
      "# Original list:  [64, 34, 25, 12, 22, 11, 90]\n",
      "# Sorted list:  [11, 12, 22, 25, 34, 64, 90]\n",
      "\n",
      "# This program uses the built-in sort() function in Python to sort the list.\n",
      "# The sort() function sorts the elements of a given list in a specific order - Ascending or Descending.\n",
      "# The default order is ascending.\n",
      "# The sort() function modifies the list it is called on.\n",
      "# The sort() function does not return a new list, it sorts the list it is called on.\n",
      "# The sort() function can take an optional key parameter, which is a function to serve as a key or a basis of sort comparison.\n",
      "# The key parameter is a function of one argument and is used to extract a comparison key from each element in the list.\n",
      "# The sort() function can take an optional reverse parameter. If it is true, the list elements are sorted in reverse order.\n",
      "# The sort() function can take an optional key and reverse parameters.\n",
      "# The sort() function can take a tuple as a key parameter.\n",
      "# The sort() function can take a tuple as a reverse parameter.\n",
      "# The sort() function can take a list as a key parameter.\n",
      "# The sort() function can take a list as a reverse parameter.\n",
      "# The sort() function can take a tuple as a key and reverse parameters.\n",
      "# The sort() function can take a list as a key and reverse parameters.\n",
      "# The sort() function can take a tuple as a key and a list as a reverse parameter.\n",
      "# The sort() function can take a list as a key and a tuple as a reverse parameter.\n",
      "# The sort() function can take a tuple as a key and a list as a reverse parameter.\n",
      "# The sort() function can take a list as a key and a tuple as a reverse parameter.\n",
      "# The sort() function can take a tuple as a key and a list as a reverse parameter.\n",
      "# The sort() function can take a list as a key and a tuple as a reverse parameter.\n",
      "# The sort() function can take a tuple as a key and a list as a reverse parameter.\n",
      "# The sort() function can take a list as a key and a tuple as a reverse parameter.\n",
      "# The sort() function can take a tuple as a key and a list as a reverse parameter.\n",
      "# The sort() function can take a list as a key and a tuple as a reverse parameter.\n",
      "# The sort() function can take a tuple as a key and a list as a reverse parameter.\n",
      "# The sort() function can take a list as a key and a tuple as a reverse parameter.\n",
      "# The sort() function can take a tuple as a key and a list as a reverse parameter.\n",
      "# The sort() function can take a list as a key and a tuple as a reverse parameter.\n",
      "# The sort() function can take a tuple as a key and a list as a reverse parameter.\n",
      "# The sort() function can take a list as a key and a tuple as a reverse parameter.\n",
      "# The sort() function can take a tuple as a key and a list as a reverse parameter.\n",
      "# The sort() function can take a list as a key and a tuple as a reverse parameter.\n",
      "# The sort() function can take a tuple as a key and a list as a reverse parameter.\n",
      "# The sort() function can take a list as a key and a tuple as a reverse parameter.\n",
      "# The sort() function can take a tuple as a key and a list as a reverse parameter.\n",
      "# The sort() function can take a list as a key and a tuple as a reverse parameter.\n",
      "# The sort() function can take a tuple as a key and a list as a reverse parameter.\n",
      "# The sort() function can take a list as a key and a tuple as a reverse parameter.\n",
      "# The sort() function can take a tuple as a key and a list as\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def generate_response(input_text):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_length=1000)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print(generate_response(\"Write a python code to sort a list!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original list:  [64, 34, 25, 12, 22, 11, 90]\n",
      "Sorted list:  [11, 12, 22, 25, 34, 64, 90]\n"
     ]
    }
   ],
   "source": [
    "def sort_list(input_list):\n",
    "    # Using built-in sort function\n",
    "    input_list.sort()\n",
    "    return input_list\n",
    "\n",
    "# Test the function\n",
    "numbers = [64, 34, 25, 12, 22, 11, 90]\n",
    "print(\"Original list: \", numbers)\n",
    "print(\"Sorted list: \", sort_list(numbers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://7b09dbd472c1c46407.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://7b09dbd472c1c46407.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "gr.Interface(\n",
    "    fn=generate_response,\n",
    "    inputs=gr.Textbox(label=\"Enter your prompt\"),\n",
    "    outputs=gr.Textbox(label=\"DeepSeek response\"),\n",
    "    title=\"Deepseek Code Generator\",\n",
    "    description=\"Ask a coding instruction and Deepseek-AI will generate a solution.\"\n",
    ").queue().launch(share=True, inline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original list:  [64, 34, 5, 12, 22, 1, 90]\n",
      "Sorted list:  [1, 5, 12, 22, 34, 64, 90]\n"
     ]
    }
   ],
   "source": [
    "def sort_list(input_list):\n",
    "    # Using built-in sort function\n",
    "    input_list.sort()\n",
    "    return input_list\n",
    "\n",
    "# Test the function\n",
    "numbers = [64, 34, 5, 12, 22, 1, 90]\n",
    "print(\"Original list: \", numbers)\n",
    "print(\"Sorted list: \", sort_list(numbers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
