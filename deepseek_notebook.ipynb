{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer from path!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os, torch\n",
    "\n",
    "model_name = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
    "model_path = os.path.join(\"models\", model_name)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    print(\"Downloading model and tokenizer from Huggingface....\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
    "    model.save_pretrained(model_path, from_pt=True)\n",
    "    tokenizer.save_pretrained(model_path)\n",
    "    print(\"Model and tokenizer downloaded and save to: \", model_path)\n",
    "else:\n",
    "    print(\"Loading model and tokenizer from path!\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a python code to sort a list!\n",
      "\n",
      "\n",
      "def sort_list(input_list):\n",
      "    return sorted(input_list)\n",
      "\n",
      "input_list = [5, 3, 2, 1, 4]\n",
      "print(sort_list(input_list))\n",
      "\n",
      "# Output: [1, 2, 3, 4, 5]\n",
      "\n",
      "# This code uses the built-in sorted() function in Python, which returns a new sorted list from the elements of any sequence.\n",
      "# The sorted() function can take a key parameter which can be set to a function of one argument that is used to extract a comparison key from each element in the list.\n",
      "# In this case, we don't need to provide a key function, so it defaults to sorting the elements as strings.\n",
      "# If you want to sort the numbers, you can provide a key function like this:\n",
      "\n",
      "def sort_list(input_list):\n",
      "    return sorted(input_list, key=lambda x: (x % 2, x))\n",
      "\n",
      "input_list = [5, 3, 2, 1, 4]\n",
      "print(sort_list(input_list))\n",
      "\n",
      "# Output: [2, 4, 1, 3, 5]\n",
      "# This code sorts the list first by the first element of each tuple (x % 2), and then by the second element of each tuple (x).\n",
      "# The tuple (x % 2) is used to sort the numbers in the list firstly by their parity (even or odd), and then by their actual values.\n",
      "# If you want to sort the numbers in descending order, you can use the reverse parameter in the sorted() function:\n",
      "\n",
      "def sort_list(input_list):\n",
      "    return sorted(input_list, key=lambda x: (x % 2, x), reverse=True)\n",
      "\n",
      "input_list = [5, 3, 2, 1, 4]\n",
      "print(sort_list(input_list))\n",
      "\n",
      "# Output: [5, 3, 1, 4, 2]\n",
      "# This code sorts the list first by the first element of each tuple (x % 2), and then by the second element of each tuple (x).\n",
      "# The tuple (x % 2) is used to sort the numbers in the list firstly by their parity (even or odd), and then by their actual values.\n",
      "# The reverse parameter is set to True, so the list is sorted in descending order.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def generate_response(input_text):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_length=1000)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print(generate_response(\"Write a python code to sort a list!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original list:  [64, 34, 25, 12, 22, 11, 90]\n",
      "Sorted list:  [11, 12, 22, 25, 34, 64, 90]\n"
     ]
    }
   ],
   "source": [
    "def sort_list(input_list):\n",
    "    # Using built-in sort function\n",
    "    input_list.sort()\n",
    "    return input_list\n",
    "\n",
    "# Test the function\n",
    "numbers = [64, 34, 25, 12, 22, 11, 90]\n",
    "print(\"Original list: \", numbers)\n",
    "print(\"Sorted list: \", sort_list(numbers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://7b09dbd472c1c46407.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://7b09dbd472c1c46407.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "gr.Interface(\n",
    "    fn=generate_response,\n",
    "    inputs=gr.Textbox(label=\"Enter your prompt\"),\n",
    "    outputs=gr.Textbox(label=\"DeepSeek response\"),\n",
    "    title=\"Deepseek Code Generator\",\n",
    "    description=\"Ask a coding instruction and Deepseek-AI will generate a solution.\"\n",
    ").queue().launch(share=True, inline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original list:  [64, 34, 5, 12, 22, 1, 90]\n",
      "Sorted list:  [1, 5, 12, 22, 34, 64, 90]\n"
     ]
    }
   ],
   "source": [
    "def sort_list(input_list):\n",
    "    # Using built-in sort function\n",
    "    input_list.sort()\n",
    "    return input_list\n",
    "\n",
    "# Test the function\n",
    "numbers = [64, 34, 5, 12, 22, 1, 90]\n",
    "print(\"Original list: \", numbers)\n",
    "print(\"Sorted list: \", sort_list(numbers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
